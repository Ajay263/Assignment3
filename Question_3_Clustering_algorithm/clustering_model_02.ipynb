{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import pickle\n",
    "import joblib\n",
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "# Step 1: Load and prepare the data\n",
    "print(\"Loading data...\")\n",
    "df = pd.read_csv('webscraped_dataset.csv')\n",
    "\n",
    "# Display dataset info\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "\n",
    "# Check for missing values in article_content\n",
    "missing_content = df['article_content'].isna().sum()\n",
    "print(f\"Missing article_content values: {missing_content} ({missing_content/len(df):.2%})\")\n",
    "\n",
    "# Filter out rows with missing article_content if needed\n",
    "if missing_content > 0:\n",
    "    df = df.dropna(subset=['article_content'])\n",
    "    print(f\"Dataset shape after removing rows with missing content: {df.shape}\")\n",
    "\n",
    "# Step 2: Text preprocessing function\n",
    "def clean_text(text):\n",
    "    if isinstance(text, str):\n",
    "        # Remove URLs\n",
    "        text = re.sub(r'https?://\\S+', '', text)\n",
    "        # Remove special characters and preserve spaces\n",
    "        text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "        # Remove extra whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        # Convert to lowercase\n",
    "        text = text.lower().strip()\n",
    "        return text\n",
    "    return ''\n",
    "\n",
    "# Clean the article content - THIS IS THE MAIN CHANGE, using article_content instead of headline\n",
    "print(\"Cleaning article content...\")\n",
    "df['cleaned_content'] = df['article_content'].apply(clean_text)\n",
    "\n",
    "# Add domain-specific stopwords that don't help determine the category\n",
    "custom_stopwords = [\n",
    "    'said', 'says', 'according', 'reported', 'reuters', 'ap', 'news',\n",
    "    'report', 'today', 'yesterday', 'week', 'month', 'year', 'day',\n",
    "    'told', 'announced', 'statement', 'released', 'published', 'posted',\n",
    "    'wrote', 'article', 'story', 'comment', 'update', 'copyright'\n",
    "]\n",
    "\n",
    "# Step 3: Create TF-IDF features from article content\n",
    "print(\"Creating TF-IDF features from article content...\")\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=10000,  # Using more features since articles have more content\n",
    "    min_df=2,            # Term must appear in at least 2 documents\n",
    "    max_df=0.85,         # Ignore terms that appear in more than 85% of documents\n",
    "    ngram_range=(1, 2),  # Include single words and bigrams\n",
    "    stop_words=list(set(['english'] + custom_stopwords))\n",
    ")\n",
    "\n",
    "X = vectorizer.fit_transform(df['cleaned_content'])\n",
    "print(f\"TF-IDF matrix shape: {X.shape}\")\n",
    "\n",
    "# Step 4: Dimensionality reduction for faster processing\n",
    "print(\"Reducing dimensions with TruncatedSVD...\")\n",
    "n_components = min(300, X.shape[1] - 1)  # Don't use more components than we have features\n",
    "svd = TruncatedSVD(n_components=n_components)\n",
    "X_reduced = svd.fit_transform(X)\n",
    "print(f\"Explained variance: {svd.explained_variance_ratio_.sum():.2%}\")\n",
    "\n",
    "# Step 5: Apply K-Means clustering\n",
    "# We want 4 specific categories: Business, Politics, Arts/Culture/Celebrities, Sports\n",
    "num_clusters = 4\n",
    "print(f\"Applying K-Means clustering with {num_clusters} clusters...\")\n",
    "\n",
    "# Try multiple initializations to get the best clustering\n",
    "best_kmeans = None\n",
    "best_score = -1\n",
    "\n",
    "for i in range(10):  # Try 10 different initializations\n",
    "    kmeans = KMeans(n_clusters=num_clusters, random_state=i, n_init=10)\n",
    "    clusters = kmeans.fit_predict(X_reduced)\n",
    "    \n",
    "    # Calculate silhouette score\n",
    "    score = silhouette_score(X_reduced, clusters)\n",
    "    print(f\"Initialization {i+1}/10: Silhouette Score = {score:.4f}\")\n",
    "    \n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_kmeans = kmeans\n",
    "\n",
    "kmeans = best_kmeans\n",
    "print(f\"Best silhouette score: {best_score:.4f}\")\n",
    "\n",
    "# Assign clusters to each article\n",
    "df['cluster'] = kmeans.predict(X_reduced)\n",
    "\n",
    "# Step 6: Analyze the clusters in depth\n",
    "def get_top_terms_per_cluster(model, vectorizer, svd, n_terms=30):\n",
    "    # Get components from SVD to map back to original features\n",
    "    original_space_centroids = svd.inverse_transform(model.cluster_centers_)\n",
    "    \n",
    "    # Get the most important terms for each cluster\n",
    "    order_centroids = original_space_centroids.argsort()[:, ::-1]\n",
    "    terms = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    cluster_terms = {}\n",
    "    for i in range(model.n_clusters):\n",
    "        top_terms = [terms[ind] for ind in order_centroids[i, :n_terms]]\n",
    "        cluster_terms[i] = top_terms\n",
    "    \n",
    "    return cluster_terms\n",
    "\n",
    "print(\"\\nExtracting top terms for each cluster...\")\n",
    "top_terms = get_top_terms_per_cluster(kmeans, vectorizer, svd)\n",
    "\n",
    "print(\"\\nTop terms in each cluster:\")\n",
    "for cluster, terms in top_terms.items():\n",
    "    print(f\"Cluster {cluster}: {', '.join(terms)}\")\n",
    "\n",
    "# Step 7: Check the distribution of articles across clusters\n",
    "cluster_distribution = df['cluster'].value_counts().sort_index()\n",
    "print(\"\\nDistribution of articles across clusters:\")\n",
    "print(cluster_distribution)\n",
    "\n",
    "# Step 8: Visualize clusters using t-SNE\n",
    "print(\"\\nVisualizing clusters with t-SNE (this may take a while)...\")\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000)\n",
    "X_tsne = tsne.fit_transform(X_reduced)\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "scatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=df['cluster'], cmap='viridis', alpha=0.7, s=50)\n",
    "plt.colorbar(scatter, label='Cluster')\n",
    "plt.title('News Articles Clustered by Content', fontsize=15)\n",
    "plt.xlabel('t-SNE dimension 1')\n",
    "plt.ylabel('t-SNE dimension 2')\n",
    "plt.tight_layout()\n",
    "plt.savefig('article_clusters_visualization.png')\n",
    "print(\"Cluster visualization saved as 'article_clusters_visualization.png'\")\n",
    "\n",
    "# Step 9: Determine category for each cluster based on top terms\n",
    "# This is a critical step - we'll see samples from each cluster to help map them\n",
    "print(\"\\nAnalyzing sample articles from each cluster:\")\n",
    "for cluster in range(num_clusters):\n",
    "    cluster_samples = df[df['cluster'] == cluster].head(3)\n",
    "    print(f\"\\nCluster {cluster} samples:\")\n",
    "    for idx, row in cluster_samples.iterrows():\n",
    "        print(f\"- Headline: {row['headline']}\")\n",
    "        # Print first 100 chars of content as a preview\n",
    "        content_preview = row['article_content'][:100].replace('\\n', ' ').strip() + '...'\n",
    "        print(f\"  Content preview: {content_preview}\")\n",
    "\n",
    "# Step 10: Map clusters to categories\n",
    "# This mapping should be updated based on the analysis of cluster contents\n",
    "print(\"\\nAssigning category labels to clusters...\")\n",
    "cluster_to_category = {\n",
    "    # These are placeholder assignments - update after analyzing your results\n",
    "    0: \"Business\",\n",
    "    1: \"Politics\",\n",
    "    2: \"Arts/Culture/Celebrities\",\n",
    "    3: \"Sports\"\n",
    "}\n",
    "\n",
    "# Display the mapping\n",
    "print(\"Cluster to category mapping:\")\n",
    "for cluster, category in cluster_to_category.items():\n",
    "    print(f\"Cluster {cluster} -> {category} (based on top terms: {', '.join(top_terms[cluster][:5])})\")\n",
    "\n",
    "# Assign categories to the dataset\n",
    "df['category'] = df['cluster'].map(cluster_to_category)\n",
    "\n",
    "# Step 11: Save the models\n",
    "print(\"\\nSaving models...\")\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "# Save the vectorizer\n",
    "with open('models/tfidf_vectorizer.pkl', 'wb') as f:\n",
    "    pickle.dump(vectorizer, f)\n",
    "\n",
    "# Save the SVD model\n",
    "joblib.dump(svd, 'models/svd_model.joblib')\n",
    "\n",
    "# Save the kmeans model\n",
    "joblib.dump(kmeans, 'models/kmeans_model.joblib')\n",
    "\n",
    "# Save the cluster to category mapping\n",
    "with open('models/cluster_to_category.pkl', 'wb') as f:\n",
    "    pickle.dump(cluster_to_category, f)\n",
    "\n",
    "# Save categorized data\n",
    "output_df = df[['headline', 'article_title', 'article_url', 'cluster', 'category']]\n",
    "output_df.to_csv('categorized_news.csv', index=False)\n",
    "print(\"Categorized news saved to 'categorized_news.csv'\")\n",
    "\n",
    "# Step 12: Create a function to predict categories for new headlines or articles\n",
    "def predict_category(text, vectorizer, svd, kmeans_model, cluster_mapping):\n",
    "    \"\"\"Predict the category of a news article or headline based on content\"\"\"\n",
    "    # Clean the text\n",
    "    cleaned_text = clean_text(text)\n",
    "    \n",
    "    # Transform to TF-IDF features\n",
    "    text_tfidf = vectorizer.transform([cleaned_text])\n",
    "    \n",
    "    # Apply dimensionality reduction\n",
    "    text_reduced = svd.transform(text_tfidf)\n",
    "    \n",
    "    # Predict the cluster\n",
    "    cluster = kmeans_model.predict(text_reduced)[0]\n",
    "    \n",
    "    # Map the cluster to a category\n",
    "    category = cluster_mapping.get(cluster, \"Unknown\")\n",
    "    \n",
    "    return category, cluster\n",
    "\n",
    "# Step 13: Test with example headlines and articles from each expected category\n",
    "test_content = {\n",
    "    \"Business\": [\n",
    "        \"The stock market plunged today as investors reacted to rising inflation numbers. The Dow Jones Industrial Average fell by over 500 points, while the NASDAQ saw a 3% decline. Tech stocks were particularly hard hit, with major companies like Apple and Microsoft seeing significant drops in share value. Economic analysts suggest that this market correction reflects growing concerns about the Federal Reserve's potential interest rate hikes.\",\n",
    "        \"The merger between two major retail chains was approved by regulators today. The $4.2 billion deal will create the largest department store company in the country, with over 500 locations nationwide. Shareholders from both companies overwhelmingly supported the merger, which is expected to generate cost savings of approximately $500 million annually through combined operations and supply chain efficiencies.\"\n",
    "    ],\n",
    "    \"Politics\": [\n",
    "        \"The President delivered a major policy speech today outlining his administration's legislative agenda for the coming year. Key priorities include infrastructure investment, climate change initiatives, and healthcare reform. Opposition leaders were quick to criticize the proposals, calling them too expensive and overreaching. Political analysts note that the success of this agenda will largely depend on gaining support from moderate senators in the upcoming vote.\",\n",
    "        \"Election officials have certified the results of last month's gubernatorial race after completing a mandatory recount. The final tally showed the challenger winning by just 1,200 votes out of more than 2.3 million ballots cast. This marks the closest gubernatorial election in the state's history and ends weeks of legal challenges and vote verification procedures.\"\n",
    "    ],\n",
    "    \"Arts/Culture/Celebrities\": [\n",
    "        \"The acclaimed director's latest film received a standing ovation at its festival premiere last night. Critics are already praising the cinematography and powerful performances from the ensemble cast. The three-hour drama, which explores themes of family and identity, is expected to be a major contender during awards season. The film's lead actress, who underwent a physical transformation for the role, is being singled out for particular acclaim.\",\n",
    "        \"The pop star surprised fans yesterday by releasing an unannounced album at midnight. The 14-track collection features collaborations with several prominent artists and represents a significant departure from her previous musical style. Social media has been flooded with reactions from fans and critics alike, with many praising the artist's willingness to experiment with new sounds and personal lyrical themes.\"\n",
    "    ],\n",
    "    \"Sports\": [\n",
    "        \"The underdog team completed their Cinderella run last night, winning the championship in a thrilling overtime victory. The team's star player scored the winning points with just 3 seconds remaining on the clock, capping off a remarkable comeback from a 15-point deficit. This marks the franchise's first title in their 50-year history and sets up a celebration parade scheduled for this weekend in the downtown area.\",\n",
    "        \"The veteran quarterback announced his retirement today after 18 seasons in the league. During his career, he led his teams to three championships and was selected for the all-star game seven times. Team officials and former teammates attended the emotional press conference where he thanked fans and reflected on his achievements. The team is expected to retire his jersey number in a ceremony planned for next season's home opener.\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"\\n--- MODEL EVALUATION ---\")\n",
    "correct_predictions = 0\n",
    "total_predictions = 0\n",
    "\n",
    "for expected_category, contents in test_content.items():\n",
    "    print(f\"\\nTesting {expected_category} content:\")\n",
    "    for i, content in enumerate(contents):\n",
    "        predicted_category, cluster = predict_category(content, vectorizer, svd, kmeans, cluster_to_category)\n",
    "        is_correct = predicted_category == expected_category\n",
    "        \n",
    "        if is_correct:\n",
    "            correct_predictions += 1\n",
    "        \n",
    "        total_predictions += 1\n",
    "        \n",
    "        print(f\"Example {i+1}:\")\n",
    "        # Print just the first 100 characters of the test content\n",
    "        content_preview = content[:100].replace('\\n', ' ').strip() + \"...\"\n",
    "        print(f\"Content preview: {content_preview}\")\n",
    "        print(f\"Predicted: {predicted_category} (Cluster {cluster})\")\n",
    "        print(f\"Expected: {expected_category}\")\n",
    "        print(f\"Correct: {'✓' if is_correct else '✗'}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "overall_accuracy = correct_predictions / total_predictions if total_predictions else 0\n",
    "print(f\"\\nOverall accuracy: {overall_accuracy:.2f} ({correct_predictions}/{total_predictions})\")\n",
    "\n",
    "# Step 14: Create a simple function for headline classification\n",
    "def headline_classifier():\n",
    "    \"\"\"Interactive function to classify headlines or content\"\"\"\n",
    "    print(\"\\n--- HEADLINE & CONTENT CLASSIFIER ---\")\n",
    "    print(\"Enter text to classify (type 'quit' to exit):\")\n",
    "    \n",
    "    while True:\n",
    "        text = input(\"\\nEnter headline or article content: \")\n",
    "        if text.lower() == 'quit':\n",
    "            break\n",
    "        \n",
    "        category, cluster = predict_category(text, vectorizer, svd, kmeans, cluster_to_category)\n",
    "        print(f\"Predicted Category: {category} (Cluster {cluster})\")\n",
    "        print(f\"Top terms in this cluster: {', '.join(top_terms[cluster][:10])}\")\n",
    "\n",
    "# Run the interactive classifier\n",
    "headline_classifier()\n",
    "\n",
    "print(\"\\nModel training and evaluation complete!\")\n",
    "print(\"To classify new headlines or articles, you can:\")\n",
    "print(\"1. Run this script again and use the interactive classifier at the end\")\n",
    "print(\"2. Import the classifier function from another script\")\n",
    "print(\"3. Use the models saved in the 'models/' directory with your own code\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
